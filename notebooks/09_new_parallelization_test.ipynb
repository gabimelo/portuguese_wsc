{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.consts import *\n",
    "from src.main import main, setup_torch, get_corpus\n",
    "from src.model import RNNModel\n",
    "from src.training import train, evaluate\n",
    "from src.split_cross_entropy_loss import SplitCrossEntropyLoss\n",
    "from src.parallel import DataParallelModel, DataParallelCriterion\n",
    "from src.custom_data_parallel import CustomDataParallel\n",
    "\n",
    "from notebooks.utils import summary, check_cuda_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data_paralellization = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomDataParallel(\n",
      "  (model): DataParallelModel(\n",
      "    (module): RNNModel(\n",
      "      (drop): Dropout(p=0.2)\n",
      "      (encoder): Embedding(602755, 200)\n",
      "      (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)\n",
      "      (decoder): Linear(in_features=200, out_features=602755, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "model.module.encoder.weight torch.Size([602755, 200])\n",
      "model.module.rnn.weight_ih_l0 torch.Size([800, 200])\n",
      "model.module.rnn.weight_hh_l0 torch.Size([800, 200])\n",
      "model.module.rnn.bias_ih_l0 torch.Size([800])\n",
      "model.module.rnn.bias_hh_l0 torch.Size([800])\n",
      "model.module.rnn.weight_ih_l1 torch.Size([800, 200])\n",
      "model.module.rnn.weight_hh_l1 torch.Size([800, 200])\n",
      "model.module.rnn.bias_ih_l1 torch.Size([800])\n",
      "model.module.rnn.bias_hh_l1 torch.Size([800])\n",
      "model.module.decoder.weight torch.Size([602755, 200])\n",
      "model.module.decoder.bias torch.Size([602755])\n",
      "\n",
      "Total Parameters: 121,796,955\n"
     ]
    }
   ],
   "source": [
    "setup_torch()\n",
    "# torch.cuda.set_device(1)\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "corpus = get_corpus()\n",
    "\n",
    "# TODO remove these two lines\n",
    "assert len(corpus.dictionary) == 602755\n",
    "assert corpus.valid.size()[0] == 11606861\n",
    "assert corpus.train.max() < len(corpus.dictionary)\n",
    "assert corpus.valid.max() < len(corpus.dictionary)\n",
    "assert corpus.test.max() < len(corpus.dictionary)\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(MODEL_TYPE, ntokens, EMBEDDINGS_SIZE, HIDDEN_UNIT_COUNT, LAYER_COUNT, DROPOUT_PROB,\n",
    "                 TIED).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if use_data_paralellization or USE_DATA_PARALLELIZATION:\n",
    "    model = CustomDataParallel(model)\n",
    "    criterion = DataParallelCriterion(criterion)\n",
    "# else:\n",
    "#     model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "summary(model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside forwardinside forward torch.Size([350, 602755])\n",
      " torch.Size([350, 602755])\n",
      "torch.Size([10, 2, 200])\n",
      "torch.Size([10, 2, 200])\n",
      "torch.Size([10, 2, 200])\n",
      "torch.Size([10, 2, 200])\n",
      "device index:  0\n",
      "torch.Size([350, 602755])\n",
      "torch.Size([350])\n",
      "device index:  1\n",
      "torch.Size([350, 602755])\n",
      "torch.Size([350])\n",
      "tensor(13.3070, device='cuda:1', grad_fn=<NllLossBackward>)\n",
      "tensor(13.3074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "> \u001b[0;32m/media/gabrielamelo/Novo volume/Projects/portuguese_wsc/src/training.py\u001b[0m(137)\u001b[0;36mtrain_one_epoch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    136 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 137 \u001b[0;31m        \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    138 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside forward torch.Size([350, 602755])\n",
      "torch.Size([10, 2, 200])\n",
      "torch.Size([10, 2, 200])\n",
      "inside forward torch.Size([350, 602755])\n",
      "torch.Size([10, 2, 200])\n",
      "torch.Size([10, 2, 200])\n",
      "device index: device index:  1\n",
      "torch.Size([350, 602755]) 0\n",
      "torch.Size([350, 602755])\n",
      "torch.Size([350])\n",
      "\n",
      "torch.Size([350])\n",
      "tensor(13.3012, device='cuda:1', grad_fn=<NllLossBackward>)\n",
      "tensor(13.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 804.88 MiB (GPU 0; 10.91 GiB total capacity; 8.73 GiB already allocated; 474.94 MiB free; 521.52 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2cca7ed578c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/gabrielamelo/Novo volume/Projects/portuguese_wsc/src/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, corpus, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/gabrielamelo/Novo volume/Projects/portuguese_wsc/src/training.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, corpus, criterion, optimizer, lr, epoch, device)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mLOG_INTERVAL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wsc_port/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wsc_port/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 804.88 MiB (GPU 0; 10.91 GiB total capacity; 8.73 GiB already allocated; 474.94 MiB free; 521.52 MiB cached)"
     ]
    }
   ],
   "source": [
    "train(model, corpus, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp = datetime.datetime.now()\n",
    "# with open(MODEL_FILE_NAME.format(timestamp), 'wb') as f:\n",
    "#     torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(MODEL_FILE_NAME.format(timestamp), 'rb') as f:\n",
    "with open('models/trained_models/model-2019-05-24 17:19:46.971655.pt', 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    model.rnn.flatten_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2019-05-27 10:57:05,109: -----------------------------------------------------------------------------------------\n",
      "INFO 2019-05-27 10:57:05,110: Running eval\n",
      "INFO 2019-05-27 10:57:05,110: -----------------------------------------------------------------------------------------\n",
      "INFO 2019-05-27 10:57:31,470: |  1000/42211 batches | loss 175.47\n",
      "INFO 2019-05-27 10:57:57,898: |  2000/42211 batches | loss 175.93\n",
      "INFO 2019-05-27 10:58:24,412: |  3000/42211 batches | loss 176.24\n",
      "INFO 2019-05-27 10:58:50,994: |  4000/42211 batches | loss 176.12\n",
      "INFO 2019-05-27 10:59:17,711: |  5000/42211 batches | loss 175.86\n",
      "INFO 2019-05-27 10:59:44,515: |  6000/42211 batches | loss 175.90\n",
      "INFO 2019-05-27 11:00:11,298: |  7000/42211 batches | loss 175.92\n",
      "INFO 2019-05-27 11:00:38,091: |  8000/42211 batches | loss 176.16\n",
      "INFO 2019-05-27 11:01:04,898: |  9000/42211 batches | loss 176.03\n",
      "INFO 2019-05-27 11:01:31,771: | 10000/42211 batches | loss 176.04\n",
      "INFO 2019-05-27 11:01:58,692: | 11000/42211 batches | loss 175.98\n",
      "INFO 2019-05-27 11:02:25,636: | 12000/42211 batches | loss 175.79\n",
      "INFO 2019-05-27 11:02:52,621: | 13000/42211 batches | loss 176.28\n",
      "INFO 2019-05-27 11:03:19,625: | 14000/42211 batches | loss 176.09\n",
      "INFO 2019-05-27 11:03:46,666: | 15000/42211 batches | loss 175.91\n",
      "INFO 2019-05-27 11:06:28,953: | 21000/42211 batches | loss 175.49\n",
      "INFO 2019-05-27 11:06:56,057: | 22000/42211 batches | loss 175.48\n",
      "INFO 2019-05-27 11:07:23,091: | 23000/42211 batches | loss 175.46\n",
      "INFO 2019-05-27 11:07:50,256: | 24000/42211 batches | loss 175.49\n",
      "INFO 2019-05-27 11:08:17,323: | 25000/42211 batches | loss 175.52\n",
      "INFO 2019-05-27 11:08:44,392: | 26000/42211 batches | loss 175.58\n",
      "INFO 2019-05-27 11:09:11,445: | 27000/42211 batches | loss 175.67\n",
      "INFO 2019-05-27 11:09:38,418: | 28000/42211 batches | loss 175.73\n",
      "INFO 2019-05-27 11:10:05,249: | 29000/42211 batches | loss 175.78\n",
      "INFO 2019-05-27 11:10:32,054: | 30000/42211 batches | loss 175.86\n",
      "INFO 2019-05-27 11:10:58,873: | 31000/42211 batches | loss 175.93\n",
      "INFO 2019-05-27 11:11:25,678: | 32000/42211 batches | loss 175.88\n",
      "INFO 2019-05-27 11:11:52,473: | 33000/42211 batches | loss 175.90\n",
      "INFO 2019-05-27 11:12:19,316: | 34000/42211 batches | loss 175.94\n",
      "INFO 2019-05-27 11:12:46,221: | 35000/42211 batches | loss 176.05\n",
      "INFO 2019-05-27 11:13:13,208: | 36000/42211 batches | loss 176.09\n",
      "INFO 2019-05-27 11:13:40,229: | 37000/42211 batches | loss 176.02\n",
      "INFO 2019-05-27 11:14:07,277: | 38000/42211 batches | loss 176.02\n",
      "INFO 2019-05-27 11:14:34,259: | 39000/42211 batches | loss 176.14\n",
      "INFO 2019-05-27 11:15:01,315: | 40000/42211 batches | loss 176.25\n",
      "INFO 2019-05-27 11:15:28,366: | 41000/42211 batches | loss 176.22\n",
      "INFO 2019-05-27 11:15:55,734: | 42000/42211 batches | loss 176.29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.038112595037704"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, corpus, criterion, device, use_test_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsc_port",
   "language": "python",
   "name": "wsc_port"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
