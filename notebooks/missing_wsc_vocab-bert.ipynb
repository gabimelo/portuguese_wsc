{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0203 23:05:34.518196 139961860638464 file_utils.py:35] PyTorch version 1.0.1.post2 available.\n",
      "W0203 23:05:35.300997 139961860638464 __init__.py:28] To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from src.winograd_collection_manipulation.wsc_json_handler import generate_df_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hashtags(vocab):\n",
    "    count = 0\n",
    "    for i in list(vocab):\n",
    "        if '##' in i:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def get_bert_wsc_vocab(model_file_name):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_file_name)\n",
    "    text_columns = df.loc[:, (df.applymap(type) == str).all(axis=0)].columns\n",
    "    wsc_vocab = set(df[text_columns].applymap(tokenizer.tokenize).sum().sum())\n",
    "    \n",
    "    print(tokenizer.special_tokens_map['unk_token'] in wsc_vocab)\n",
    "    \n",
    "    return wsc_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_df_from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.translated].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct_sentence</th>\n",
       "      <th>incorrect_sentence</th>\n",
       "      <th>manually_fixed_correct_sentence</th>\n",
       "      <th>manually_fixed_incorrect_sentence</th>\n",
       "      <th>correct_switched</th>\n",
       "      <th>incorrect_switched</th>\n",
       "      <th>is_switchable</th>\n",
       "      <th>is_associative</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Os vereadores recusaram a autorização aos mani...</td>\n",
       "      <td>Os vereadores recusaram a autorização aos mani...</td>\n",
       "      <td>Os vereadores recusaram a autorização aos mani...</td>\n",
       "      <td>Os vereadores recusaram a autorização aos mani...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Os vereadores recusaram a autorização aos mani...</td>\n",
       "      <td>Os vereadores recusaram a autorização aos mani...</td>\n",
       "      <td>Os vereadores recusaram a autorização aos mani...</td>\n",
       "      <td>Os vereadores recusaram a autorização aos mani...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A medalha não cabe na maleta porque a medalha ...</td>\n",
       "      <td>A medalha não cabe na maleta porque a maleta é...</td>\n",
       "      <td>A medalha não cabe na maleta porque a medalha ...</td>\n",
       "      <td>A medalha não cabe na maleta porque a maleta é...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A medalha não cabe na maleta porque a maleta é...</td>\n",
       "      <td>A medalha não cabe na maleta porque a medalha ...</td>\n",
       "      <td>A medalha não cabe na maleta porque a maleta é...</td>\n",
       "      <td>A medalha não cabe na maleta porque a medalha ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joan certificou-se de agradecer Susan por toda...</td>\n",
       "      <td>Joan certificou-se de agradecer Susan por toda...</td>\n",
       "      <td>Joan certificou-se de agradecer Susan por toda...</td>\n",
       "      <td>Joan certificou-se de agradecer Susan por toda...</td>\n",
       "      <td>Susan certificou-se de agradecer Joan por toda...</td>\n",
       "      <td>Susan certificou-se de agradecer Joan por toda...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    correct_sentence  \\\n",
       "0  Os vereadores recusaram a autorização aos mani...   \n",
       "1  Os vereadores recusaram a autorização aos mani...   \n",
       "2  A medalha não cabe na maleta porque a medalha ...   \n",
       "3  A medalha não cabe na maleta porque a maleta é...   \n",
       "4  Joan certificou-se de agradecer Susan por toda...   \n",
       "\n",
       "                                  incorrect_sentence  \\\n",
       "0  Os vereadores recusaram a autorização aos mani...   \n",
       "1  Os vereadores recusaram a autorização aos mani...   \n",
       "2  A medalha não cabe na maleta porque a maleta é...   \n",
       "3  A medalha não cabe na maleta porque a medalha ...   \n",
       "4  Joan certificou-se de agradecer Susan por toda...   \n",
       "\n",
       "                     manually_fixed_correct_sentence  \\\n",
       "0  Os vereadores recusaram a autorização aos mani...   \n",
       "1  Os vereadores recusaram a autorização aos mani...   \n",
       "2  A medalha não cabe na maleta porque a medalha ...   \n",
       "3  A medalha não cabe na maleta porque a maleta é...   \n",
       "4  Joan certificou-se de agradecer Susan por toda...   \n",
       "\n",
       "                   manually_fixed_incorrect_sentence  \\\n",
       "0  Os vereadores recusaram a autorização aos mani...   \n",
       "1  Os vereadores recusaram a autorização aos mani...   \n",
       "2  A medalha não cabe na maleta porque a maleta é...   \n",
       "3  A medalha não cabe na maleta porque a medalha ...   \n",
       "4  Joan certificou-se de agradecer Susan por toda...   \n",
       "\n",
       "                                    correct_switched  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  Susan certificou-se de agradecer Joan por toda...   \n",
       "\n",
       "                                  incorrect_switched  is_switchable  \\\n",
       "0                                                             False   \n",
       "1                                                             False   \n",
       "2                                                             False   \n",
       "3                                                             False   \n",
       "4  Susan certificou-se de agradecer Joan por toda...           True   \n",
       "\n",
       "   is_associative  translated  \n",
       "0           False        True  \n",
       "1           False        True  \n",
       "2           False        True  \n",
       "3           False        True  \n",
       "4           False        True  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0203 23:05:35.450372 139961860638464 tokenization_utils.py:327] Model name 'models/neuralmind/bert-large-portuguese-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'models/neuralmind/bert-large-portuguese-cased' is a path or url to a directory containing tokenizer files.\n",
      "I0203 23:05:35.450755 139961860638464 tokenization_utils.py:359] Didn't find file models/neuralmind/bert-large-portuguese-cased/added_tokens.json. We won't load it.\n",
      "I0203 23:05:35.451097 139961860638464 tokenization_utils.py:359] Didn't find file models/neuralmind/bert-large-portuguese-cased/special_tokens_map.json. We won't load it.\n",
      "I0203 23:05:35.451435 139961860638464 tokenization_utils.py:359] Didn't find file models/neuralmind/bert-large-portuguese-cased/tokenizer_config.json. We won't load it.\n",
      "I0203 23:05:35.451761 139961860638464 tokenization_utils.py:395] loading file models/neuralmind/bert-large-portuguese-cased/vocab.txt\n",
      "I0203 23:05:35.452002 139961860638464 tokenization_utils.py:395] loading file None\n",
      "I0203 23:05:35.452213 139961860638464 tokenization_utils.py:395] loading file None\n",
      "I0203 23:05:35.452529 139961860638464 tokenization_utils.py:395] loading file None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'models/neuralmind/bert-large-portuguese-cased'\n",
    "wsc_vocab = get_bert_wsc_vocab(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WSC vocab 1264\n",
      "Total Missing Words 319\n",
      "Percentage Missing Words 25.24%\n"
     ]
    }
   ],
   "source": [
    "len_missing_words = count_hashtags(wsc_vocab)\n",
    "print(f'Total WSC vocab {len(wsc_vocab)}')\n",
    "print(f'Total Missing Words {len_missing_words}')\n",
    "print(f'Percentage Missing Words {len_missing_words/len(wsc_vocab)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0203 23:05:36.539514 139961860638464 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/gabrielamelo/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'bert-base-multilingual-cased'\n",
    "wsc_vocab = get_bert_wsc_vocab(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WSC vocab 1367\n",
      "Total Missing Words 483\n",
      "Percentage Missing Words 35.33%\n"
     ]
    }
   ],
   "source": [
    "len_missing_words = count_hashtags(wsc_vocab)\n",
    "print(f'Total WSC vocab {len(wsc_vocab)}')\n",
    "print(f'Total Missing Words {len_missing_words}')\n",
    "print(f'Percentage Missing Words {len_missing_words/len(wsc_vocab)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0203 23:05:37.873224 139961860638464 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at /home/gabrielamelo/.cache/torch/transformers/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'bert-large-cased'\n",
    "wsc_vocab = get_bert_wsc_vocab(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WSC vocab 985\n",
      "Total Missing Words 590\n",
      "Percentage Missing Words 59.90%\n"
     ]
    }
   ],
   "source": [
    "len_missing_words = count_hashtags(wsc_vocab)\n",
    "print(f'Total WSC vocab {len(wsc_vocab)}')\n",
    "print(f'Total Missing Words {len_missing_words}')\n",
    "print(f'Percentage Missing Words {len_missing_words/len(wsc_vocab)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import patch\n",
    "\n",
    "with patch('src.winograd_collection_manipulation.wsc_json_handler.WINOGRAD_SCHEMAS_FILE', \n",
    "        'data/processed/english_wsc.json'):\n",
    "    from src.winograd_collection_manipulation.wsc_json_handler import generate_df_from_json\n",
    "    df = generate_df_from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct_sentence</th>\n",
       "      <th>incorrect_sentence</th>\n",
       "      <th>manually_fixed_correct_sentence</th>\n",
       "      <th>manually_fixed_incorrect_sentence</th>\n",
       "      <th>correct_switched</th>\n",
       "      <th>incorrect_switched</th>\n",
       "      <th>is_switchable</th>\n",
       "      <th>is_associative</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The city councilmen refused the demonstrators ...</td>\n",
       "      <td>The city councilmen refused the demonstrators ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The city councilmen refused the demonstrators ...</td>\n",
       "      <td>The city councilmen refused the demonstrators ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The trophy doesn't fit into the brown suitcase...</td>\n",
       "      <td>The trophy doesn't fit into the brown suitcase...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The trophy doesn't fit into the brown suitcase...</td>\n",
       "      <td>The trophy doesn't fit into the brown suitcase...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joan made sure to thank Susan for all the help...</td>\n",
       "      <td>Joan made sure to thank Susan for all the help...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Susan made sure to thank Joan for all the help...</td>\n",
       "      <td>Susan made sure to thank Joan for all the help...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    correct_sentence  \\\n",
       "0  The city councilmen refused the demonstrators ...   \n",
       "1  The city councilmen refused the demonstrators ...   \n",
       "2  The trophy doesn't fit into the brown suitcase...   \n",
       "3  The trophy doesn't fit into the brown suitcase...   \n",
       "4  Joan made sure to thank Susan for all the help...   \n",
       "\n",
       "                                  incorrect_sentence  \\\n",
       "0  The city councilmen refused the demonstrators ...   \n",
       "1  The city councilmen refused the demonstrators ...   \n",
       "2  The trophy doesn't fit into the brown suitcase...   \n",
       "3  The trophy doesn't fit into the brown suitcase...   \n",
       "4  Joan made sure to thank Susan for all the help...   \n",
       "\n",
       "  manually_fixed_correct_sentence manually_fixed_incorrect_sentence  \\\n",
       "0                                                                     \n",
       "1                                                                     \n",
       "2                                                                     \n",
       "3                                                                     \n",
       "4                                                                     \n",
       "\n",
       "                                    correct_switched  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  Susan made sure to thank Joan for all the help...   \n",
       "\n",
       "                                  incorrect_switched  is_switchable  \\\n",
       "0                                                             False   \n",
       "1                                                             False   \n",
       "2                                                             False   \n",
       "3                                                             False   \n",
       "4  Susan made sure to thank Joan for all the help...           True   \n",
       "\n",
       "   is_associative  translated  \n",
       "0           False        True  \n",
       "1           False        True  \n",
       "2           False        True  \n",
       "3           False        True  \n",
       "4           False        True  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0203 23:05:39.096808 139961860638464 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at /home/gabrielamelo/.cache/torch/transformers/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'bert-large-cased'\n",
    "wsc_vocab = get_bert_wsc_vocab(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WSC vocab 1034\n",
      "Total Missing Words 103\n",
      "Percentage Missing Words 9.96%\n"
     ]
    }
   ],
   "source": [
    "len_missing_words = count_hashtags(wsc_vocab)\n",
    "print(f'Total WSC vocab {len(wsc_vocab)}')\n",
    "print(f'Total Missing Words {len_missing_words}')\n",
    "print(f'Percentage Missing Words {len_missing_words/len(wsc_vocab)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0203 23:05:39.971314 139961860638464 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/gabrielamelo/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'bert-base-multilingual-cased'\n",
    "wsc_vocab = get_bert_wsc_vocab(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WSC vocab 1094\n",
      "Total Missing Words 204\n",
      "Percentage Missing Words 18.65%\n"
     ]
    }
   ],
   "source": [
    "len_missing_words = count_hashtags(wsc_vocab)\n",
    "print(f'Total WSC vocab {len(wsc_vocab)}')\n",
    "print(f'Total Missing Words {len_missing_words}')\n",
    "print(f'Percentage Missing Words {len_missing_words/len(wsc_vocab)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0203 23:05:40.290853 139961860638464 tokenization_utils.py:327] Model name 'models/neuralmind/bert-large-portuguese-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'models/neuralmind/bert-large-portuguese-cased' is a path or url to a directory containing tokenizer files.\n",
      "I0203 23:05:40.291320 139961860638464 tokenization_utils.py:359] Didn't find file models/neuralmind/bert-large-portuguese-cased/added_tokens.json. We won't load it.\n",
      "I0203 23:05:40.291628 139961860638464 tokenization_utils.py:359] Didn't find file models/neuralmind/bert-large-portuguese-cased/special_tokens_map.json. We won't load it.\n",
      "I0203 23:05:40.291872 139961860638464 tokenization_utils.py:359] Didn't find file models/neuralmind/bert-large-portuguese-cased/tokenizer_config.json. We won't load it.\n",
      "I0203 23:05:40.292154 139961860638464 tokenization_utils.py:395] loading file models/neuralmind/bert-large-portuguese-cased/vocab.txt\n",
      "I0203 23:05:40.292379 139961860638464 tokenization_utils.py:395] loading file None\n",
      "I0203 23:05:40.292589 139961860638464 tokenization_utils.py:395] loading file None\n",
      "I0203 23:05:40.292802 139961860638464 tokenization_utils.py:395] loading file None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'models/neuralmind/bert-large-portuguese-cased'\n",
    "wsc_vocab = get_bert_wsc_vocab(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WSC vocab 907\n",
      "Total Missing Words 453\n",
      "Percentage Missing Words 49.94%\n"
     ]
    }
   ],
   "source": [
    "len_missing_words = count_hashtags(wsc_vocab)\n",
    "print(f'Total WSC vocab {len(wsc_vocab)}')\n",
    "print(f'Total Missing Words {len_missing_words}')\n",
    "print(f'Percentage Missing Words {len_missing_words/len(wsc_vocab)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsc_port",
   "language": "python",
   "name": "wsc_port"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
