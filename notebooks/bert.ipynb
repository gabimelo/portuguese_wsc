{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0127 23:54:07.764195 140057921382144 file_utils.py:35] PyTorch version 1.0.1.post2 available.\n",
      "W0127 23:54:08.549579 140057921382144 __init__.py:28] To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "\n",
    "\n",
    "def get_most_probable_following_sentence(tokenizer, model, text1, text2):\n",
    "    text1_toks = [\"[CLS]\"] + tokenizer.tokenize(text1) + [\"[SEP]\"]\n",
    "    text2_toks = tokenizer.tokenize(text2) + [\"[SEP]\"]\n",
    "    text = text1_toks+text2_toks\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(text)\n",
    "    segments_ids = [0]*len(text1_toks) + [1]*len(text2_toks)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    model.eval()\n",
    "    prediction = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    prediction=prediction[0] # tuple to tensor\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    prediction_sm = softmax(prediction)\n",
    "\n",
    "    return prediction_sm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0127 23:04:39.549858 139702180816640 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/gabrielamelo/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0127 23:04:40.129470 139702180816640 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/gabrielamelo/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I0127 23:04:40.130248 139702180816640 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0127 23:04:40.683900 139702180816640 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/gabrielamelo/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0127 23:04:42.796954 139702180816640 modeling_utils.py:483] Weights from pretrained model not used in BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "partial_get_most_probable_following_sentence = partial(get_most_probable_following_sentence, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.1673e-04, 9.9958e-01], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "text1 = \"How old are you?\"\n",
    "text2 = \"The Eiffel Tower is in Paris\"\n",
    "prediction = partial_get_most_probable_following_sentence(text1, text2)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.9999e-01, 9.6342e-06], grad_fn=<SelectBackward>)\n",
      "tensor(1.0000, grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "text1 = \"How old are you?\"\n",
    "text2 = \"I am 22 years old\"\n",
    "prediction = partial_get_most_probable_following_sentence(text1, text2)\n",
    "print(prediction)\n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0127 23:54:14.157467 140057921382144 file_utils.py:362] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpcuwu985q\n",
      "I0127 23:54:15.504419 140057921382144 file_utils.py:377] copying /tmp/tmpcuwu985q to cache at /home/gabrielamelo/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "I0127 23:54:15.506328 140057921382144 file_utils.py:381] creating metadata file for /home/gabrielamelo/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "I0127 23:54:15.507202 140057921382144 file_utils.py:390] removing temp file /tmp/tmpcuwu985q\n",
      "I0127 23:54:15.507760 140057921382144 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/gabrielamelo/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "I0127 23:54:16.161321 140057921382144 file_utils.py:362] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpblump7fp\n",
      "I0127 23:54:16.813631 140057921382144 file_utils.py:377] copying /tmp/tmpblump7fp to cache at /home/gabrielamelo/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.83b0fa3d7f1ac0e113ad300189a938c6f14d0588a4200f30eef109d0a047c484\n",
      "I0127 23:54:16.815339 140057921382144 file_utils.py:381] creating metadata file for /home/gabrielamelo/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.83b0fa3d7f1ac0e113ad300189a938c6f14d0588a4200f30eef109d0a047c484\n",
      "I0127 23:54:16.816425 140057921382144 file_utils.py:390] removing temp file /tmp/tmpblump7fp\n",
      "I0127 23:54:16.818552 140057921382144 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/gabrielamelo/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.83b0fa3d7f1ac0e113ad300189a938c6f14d0588a4200f30eef109d0a047c484\n",
      "I0127 23:54:16.819999 140057921382144 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "I0127 23:54:17.403945 140057921382144 file_utils.py:362] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp9op0506u\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "partial_get_most_probable_following_sentence = partial(get_most_probable_following_sentence, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Quantos anos você tem?\"\n",
    "text2 = \"The Eiffel Tower is in Paris\"\n",
    "prediction = partial_get_most_probable_following_sentence(text1, text2)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_breaks(first_sentence, second_sentence):\n",
    "    for i in range(len(first_sentence.split())):\n",
    "        if first_sentence.split()[i] != second_sentence.split()[i]:  # noqaE226\n",
    "            break\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_sentence_breaks():\n",
    "    first_sentence = 'The city councilmen refused the demonstrators a permit because the city councilmen feared violence.'\n",
    "    second_sentence = 'The city councilmen refused the demonstrators a permit because the demonstrators feared violence.'\n",
    "    i = get_sentence_breaks(first_sentence, second_sentence)\n",
    "    assert ' '.join(first_sentence.split()[:i]) == \\\n",
    "        'The city councilmen refused the demonstrators a permit because the'\n",
    "    assert ' '.join(second_sentence.split()[:i]) == \\\n",
    "        'The city councilmen refused the demonstrators a permit because the'\n",
    "    assert ' '.join(first_sentence.split()[i:]) == \\\n",
    "        'city councilmen feared violence.'\n",
    "    assert ' '.join(second_sentence.split()[i:]) == \\\n",
    "        'demonstrators feared violence.'\n",
    "    \n",
    "    first_sentence = 'Os vereadores recusaram a autorização aos manifestantes porque os vereadores temiam a violência.'\n",
    "    second_sentence = 'Os vereadores recusaram a autorização aos manifestantes porque os manifestantes temiam a violência.'\n",
    "    \n",
    "    i = get_sentence_breaks(first_sentence, second_sentence)\n",
    "    assert ' '.join(first_sentence.split()[:i]) == \\\n",
    "        'Os vereadores recusaram a autorização aos manifestantes porque os'\n",
    "    assert ' '.join(second_sentence.split()[:i]) == \\\n",
    "        'Os vereadores recusaram a autorização aos manifestantes porque os'\n",
    "    assert ' '.join(first_sentence.split()[i:]) == \\\n",
    "        'vereadores temiam a violência.'\n",
    "    assert ' '.join(second_sentence.split()[i:]) == \\\n",
    "        'manifestantes temiam a violência.'\n",
    "    \n",
    "test_get_sentence_breaks()\n",
    "# test with how sentences are after tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function call is different\n",
    "def analyse_single_wsc_bert(model, tokenizer, correct_sentence, wrong_sentence):\n",
    "    if correct_sentence == '' or wrong_sentence == '':\n",
    "        return False, False\n",
    "\n",
    "    i = get_sentence_breaks(correct_sentence, wrong_sentence)\n",
    "    \n",
    "    text1 = correct_sentence[:i]\n",
    "    text2 = correct_sentence[i:]\n",
    "    prob_correct_sentence_correct = get_most_probable_following_sentence(tokenizer, model, text1, text2)[0]\n",
    "\n",
    "    text1 = wrong_sentence[:i]\n",
    "    text2 = wrong_sentence[i:]\n",
    "    prob_wrong_sentence_correct = get_most_probable_following_sentence(tokenizer, model, text1, text2)[0]\n",
    "\n",
    "    result = prob_correct_sentence_correct > prob_wrong_sentence_correct\n",
    "    \n",
    "    return result, 0 # let's always return 0 for partial result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0127 23:23:43.880078 139702180816640 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/gabrielamelo/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0127 23:23:44.446662 139702180816640 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/gabrielamelo/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I0127 23:23:44.447212 139702180816640 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0127 23:23:45.000976 139702180816640 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/gabrielamelo/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0127 23:23:47.112951 139702180816640 modeling_utils.py:483] Weights from pretrained model not used in BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "def run_bert_test_for_col(df, model, tokenizer, result_col):\n",
    "    if result_col == 'original':\n",
    "        correct_column = 'correct_sentence'\n",
    "        incorrect_column = 'incorrect_sentence'\n",
    "    elif result_col == 'switched':\n",
    "        correct_column = 'correct_switched'\n",
    "        incorrect_column = 'incorrect_switched'\n",
    "    else:\n",
    "        correct_column = 'manually_fixed_correct_sentence'\n",
    "        incorrect_column = 'manually_fixed_incorrect_sentence'\n",
    "\n",
    "    partial_analyse_single_wsc_bert = partial(analyse_single_wsc_bert, model, tokenizer)\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        df.loc[i, f'{result_col}_result_full'], df.loc[i, f'{result_col}_result_partial'] = \\\n",
    "            partial_analyse_single_wsc(row[correct_column], row[incorrect_column])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winograd_test(df, corpus, model_file_name, device, model, english=False, use_bert=False):\n",
    "    df = df[df.translated].copy()\n",
    "    df = prepare_text_cols(df, corpus, english)\n",
    "    df = add_results_columns(df)\n",
    "\n",
    "    if use_bert:\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')        \n",
    "        partial_run_test_for_col = partial(run_bert_test_for_col, df, model, tokenizer)\n",
    "    else:\n",
    "        partial_run_test_for_col = partial(run_test_for_col, df, model, model_file_name, corpus, device)\n",
    "\n",
    "    df = partial_run_test_for_col(result_col='original')\n",
    "    df = partial_run_test_for_col(result_col='switched')\n",
    "\n",
    "    test_on_manually_fixed = (\n",
    "        'manually_fixed_correct_sentence' in df.columns and\n",
    "        df.iloc[0]['manually_fixed_correct_sentence'] != ''\n",
    "    )\n",
    "    if test_on_manually_fixed:\n",
    "        df = partial_run_test_for_col(result_col='manually_fixed')\n",
    "\n",
    "    metrics = calculate_metrics(df, test_on_manually_fixed)\n",
    "    generate_report(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsc_port",
   "language": "python",
   "name": "wsc_port"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
