{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabrielamelo/anaconda3/envs/wsc_port/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabrielamelo/anaconda3/envs/wsc_port/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "def next_fname(output_dir, num=0):\n",
    "    \"\"\"Get the next filename to use for writing new articles.\"\"\"\n",
    "    count = 0\n",
    "    fname = output_dir + '/' + '{:>07d}'.format(num) + '.txt'\n",
    "    return count, (num+1), fname\n",
    "\n",
    "def make_corpus(input_file, output_dir, size=10000):\n",
    "    \"\"\"Convert Wikipedia xml dump file to text corpus\"\"\"\n",
    "\n",
    "    wiki = WikiCorpus(input_file)\n",
    "    count, num, fname = next_fname(output_dir)\n",
    "    output = open(fname, 'w')\n",
    "\n",
    "    # iterate over texts and store them\n",
    "    for text in wiki.get_texts():\n",
    "        output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n",
    "        count += 1\n",
    "        if (count == size):\n",
    "            print('%s Done.' % fname)\n",
    "            output.close()\n",
    "            count, num, fname = next_fname(output_dir, num)\n",
    "            output = open(fname, 'w')\n",
    "\n",
    "    # clean up resources\n",
    "    output.close()\n",
    "    print('Completed.')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     if len(sys.argv) != 3:\n",
    "#         print('Usage: python wikicorpus.py <wikipedia_dump_file> <destination_directory> <file_size>')\n",
    "#         sys.exit(1)\n",
    "#     input      = sys.argv[1]\n",
    "#     outupt_dir = sys.argv[2]\n",
    "#     file_size  = sys.argv[3] if len(sys.argv) else None\n",
    "#     make_corpus(input, outupt_dir)\n",
    "    \n",
    "# # wikicorpus.py arwiki-20181101-pages-articles-multistream.xml.bz2 /path/to/destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/wiki_pt_splits/0000000.txt Done.\n",
      "data/wiki_pt_splits/0000001.txt Done.\n",
      "data/wiki_pt_splits/0000002.txt Done.\n",
      "data/wiki_pt_splits/0000003.txt Done.\n",
      "data/wiki_pt_splits/0000004.txt Done.\n",
      "data/wiki_pt_splits/0000005.txt Done.\n",
      "data/wiki_pt_splits/0000006.txt Done.\n",
      "data/wiki_pt_splits/0000007.txt Done.\n",
      "data/wiki_pt_splits/0000008.txt Done.\n",
      "data/wiki_pt_splits/0000009.txt Done.\n",
      "data/wiki_pt_splits/0000010.txt Done.\n",
      "data/wiki_pt_splits/0000011.txt Done.\n",
      "data/wiki_pt_splits/0000012.txt Done.\n",
      "data/wiki_pt_splits/0000013.txt Done.\n",
      "data/wiki_pt_splits/0000014.txt Done.\n",
      "data/wiki_pt_splits/0000015.txt Done.\n",
      "data/wiki_pt_splits/0000016.txt Done.\n",
      "data/wiki_pt_splits/0000017.txt Done.\n",
      "data/wiki_pt_splits/0000018.txt Done.\n",
      "data/wiki_pt_splits/0000019.txt Done.\n",
      "data/wiki_pt_splits/0000020.txt Done.\n",
      "data/wiki_pt_splits/0000021.txt Done.\n",
      "data/wiki_pt_splits/0000022.txt Done.\n",
      "data/wiki_pt_splits/0000023.txt Done.\n",
      "data/wiki_pt_splits/0000024.txt Done.\n",
      "data/wiki_pt_splits/0000025.txt Done.\n",
      "data/wiki_pt_splits/0000026.txt Done.\n",
      "data/wiki_pt_splits/0000027.txt Done.\n",
      "data/wiki_pt_splits/0000028.txt Done.\n",
      "data/wiki_pt_splits/0000029.txt Done.\n",
      "data/wiki_pt_splits/0000030.txt Done.\n",
      "data/wiki_pt_splits/0000031.txt Done.\n",
      "data/wiki_pt_splits/0000032.txt Done.\n",
      "data/wiki_pt_splits/0000033.txt Done.\n",
      "data/wiki_pt_splits/0000034.txt Done.\n",
      "data/wiki_pt_splits/0000035.txt Done.\n",
      "data/wiki_pt_splits/0000036.txt Done.\n",
      "data/wiki_pt_splits/0000037.txt Done.\n",
      "data/wiki_pt_splits/0000038.txt Done.\n",
      "data/wiki_pt_splits/0000039.txt Done.\n",
      "data/wiki_pt_splits/0000040.txt Done.\n",
      "data/wiki_pt_splits/0000041.txt Done.\n",
      "data/wiki_pt_splits/0000042.txt Done.\n",
      "data/wiki_pt_splits/0000043.txt Done.\n",
      "data/wiki_pt_splits/0000044.txt Done.\n",
      "data/wiki_pt_splits/0000045.txt Done.\n",
      "data/wiki_pt_splits/0000046.txt Done.\n",
      "data/wiki_pt_splits/0000047.txt Done.\n",
      "data/wiki_pt_splits/0000048.txt Done.\n",
      "data/wiki_pt_splits/0000049.txt Done.\n",
      "data/wiki_pt_splits/0000050.txt Done.\n",
      "data/wiki_pt_splits/0000051.txt Done.\n",
      "data/wiki_pt_splits/0000052.txt Done.\n",
      "data/wiki_pt_splits/0000053.txt Done.\n",
      "data/wiki_pt_splits/0000054.txt Done.\n",
      "data/wiki_pt_splits/0000055.txt Done.\n",
      "data/wiki_pt_splits/0000056.txt Done.\n",
      "data/wiki_pt_splits/0000057.txt Done.\n",
      "data/wiki_pt_splits/0000058.txt Done.\n",
      "data/wiki_pt_splits/0000059.txt Done.\n",
      "data/wiki_pt_splits/0000060.txt Done.\n",
      "data/wiki_pt_splits/0000061.txt Done.\n",
      "data/wiki_pt_splits/0000062.txt Done.\n",
      "data/wiki_pt_splits/0000063.txt Done.\n",
      "data/wiki_pt_splits/0000064.txt Done.\n",
      "data/wiki_pt_splits/0000065.txt Done.\n",
      "data/wiki_pt_splits/0000066.txt Done.\n",
      "data/wiki_pt_splits/0000067.txt Done.\n",
      "data/wiki_pt_splits/0000068.txt Done.\n",
      "data/wiki_pt_splits/0000069.txt Done.\n",
      "data/wiki_pt_splits/0000070.txt Done.\n",
      "data/wiki_pt_splits/0000071.txt Done.\n",
      "data/wiki_pt_splits/0000072.txt Done.\n",
      "data/wiki_pt_splits/0000073.txt Done.\n",
      "data/wiki_pt_splits/0000074.txt Done.\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "make_corpus('ptwiki-latest-pages-articles.xml.bz2', 'data/wiki_pt_splits', size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a corpus from Wikipedia dump file.\n",
    "\n",
    "Inspired by:\n",
    "https://github.com/panyang/Wikipedia_Word2vec/blob/master/v1/process_wiki.py\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "def make_corpus(in_f, out_f):\n",
    "\n",
    "\t\"\"\"Convert Wikipedia xml dump file to text corpus\"\"\"\n",
    "\n",
    "\toutput = open(out_f, 'w')\n",
    "\twiki = WikiCorpus(in_f)\n",
    "\n",
    "\ti = 0\n",
    "\tfor text in wiki.get_texts():\n",
    "\t\toutput.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n",
    "\t\ti = i + 1\n",
    "\t\tif (i % 10000 == 0):\n",
    "\t\t\tprint('Processed ' + str(i) + ' articles')\n",
    "\toutput.close()\n",
    "\tprint('Processing complete!')\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "# \tif len(sys.argv) != 3:\n",
    "# \t\tprint('Usage: python make_wiki_corpus.py <wikipedia_dump_file> <processed_text_file>')\n",
    "# \t\tsys.exit(1)\n",
    "# \tin_f = sys.argv[1]\n",
    "# \tout_f = sys.argv[2]\n",
    "# \tmake_corpus(in_f, out_f)\n",
    "    \n",
    "# python make_wiki_corpus enwiki-latest-pages-articles.xml.bz2 wiki_en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 articles\n",
      "Processed 20000 articles\n",
      "Processed 30000 articles\n",
      "Processed 40000 articles\n",
      "Processed 50000 articles\n",
      "Processed 60000 articles\n",
      "Processed 70000 articles\n",
      "Processed 80000 articles\n",
      "Processed 90000 articles\n",
      "Processed 100000 articles\n",
      "Processed 110000 articles\n",
      "Processed 120000 articles\n",
      "Processed 130000 articles\n",
      "Processed 140000 articles\n",
      "Processed 150000 articles\n",
      "Processed 160000 articles\n",
      "Processed 170000 articles\n",
      "Processed 180000 articles\n",
      "Processed 190000 articles\n",
      "Processed 200000 articles\n",
      "Processed 210000 articles\n",
      "Processed 220000 articles\n",
      "Processed 230000 articles\n",
      "Processed 240000 articles\n",
      "Processed 250000 articles\n",
      "Processed 260000 articles\n",
      "Processed 270000 articles\n",
      "Processed 280000 articles\n",
      "Processed 290000 articles\n",
      "Processed 300000 articles\n",
      "Processed 310000 articles\n",
      "Processed 320000 articles\n",
      "Processed 330000 articles\n",
      "Processed 340000 articles\n",
      "Processed 350000 articles\n",
      "Processed 360000 articles\n",
      "Processed 370000 articles\n",
      "Processed 380000 articles\n",
      "Processed 390000 articles\n",
      "Processed 400000 articles\n",
      "Processed 410000 articles\n",
      "Processed 420000 articles\n",
      "Processed 430000 articles\n",
      "Processed 440000 articles\n",
      "Processed 450000 articles\n",
      "Processed 460000 articles\n",
      "Processed 470000 articles\n",
      "Processed 480000 articles\n",
      "Processed 490000 articles\n",
      "Processed 500000 articles\n",
      "Processed 510000 articles\n",
      "Processed 520000 articles\n",
      "Processed 530000 articles\n",
      "Processed 540000 articles\n",
      "Processed 550000 articles\n",
      "Processed 560000 articles\n",
      "Processed 570000 articles\n",
      "Processed 580000 articles\n",
      "Processed 590000 articles\n",
      "Processed 600000 articles\n",
      "Processed 610000 articles\n",
      "Processed 620000 articles\n",
      "Processed 630000 articles\n",
      "Processed 640000 articles\n",
      "Processed 650000 articles\n",
      "Processed 660000 articles\n",
      "Processed 670000 articles\n",
      "Processed 680000 articles\n",
      "Processed 690000 articles\n",
      "Processed 700000 articles\n",
      "Processed 710000 articles\n",
      "Processed 720000 articles\n",
      "Processed 730000 articles\n",
      "Processed 740000 articles\n",
      "Processed 750000 articles\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "make_corpus('ptwiki-latest-pages-articles.xml.bz2', 'wiki_pt.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def load_corpus(input_file):\n",
    "    print('Loading corpus...')\n",
    "    time1 = time.time()\n",
    "    corpus = input_file.read()\n",
    "    time2 = time.time()\n",
    "    total_time = time2-time1\n",
    "    print('It took %0.3f seconds to load corpus' %total_time)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3592037be755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wiki_pt.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-7cb1d1266fb1>\u001b[0m in \u001b[0;36mload_corpus\u001b[0;34m(input_file)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading corpus...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtime1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtime2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtotal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load_corpus(open('wiki_pt.txt','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "$1",
   "language": "python",
   "name": "wsc_port"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
