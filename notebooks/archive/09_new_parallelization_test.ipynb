{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.consts import *\n",
    "from src.main import main, setup_torch, get_corpus\n",
    "from src.model import RNNModel\n",
    "from src.training import train, evaluate\n",
    "from src.split_cross_entropy_loss import SplitCrossEntropyLoss\n",
    "from src.parallel import DataParallelCriterion\n",
    "from src.custom_data_parallel import CustomDataParallel\n",
    "from src.utils import summary, check_cuda_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data_paralellization = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomDataParallel(\n",
      "  (model): DataParallelModel(\n",
      "    (module): RNNModel(\n",
      "      (drop): Dropout(p=0.2)\n",
      "      (encoder): Embedding(602755, 200)\n",
      "      (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)\n",
      "      (decoder): Linear(in_features=200, out_features=602755, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "model.module.encoder.weight torch.Size([602755, 200])\n",
      "model.module.rnn.weight_ih_l0 torch.Size([800, 200])\n",
      "model.module.rnn.weight_hh_l0 torch.Size([800, 200])\n",
      "model.module.rnn.bias_ih_l0 torch.Size([800])\n",
      "model.module.rnn.bias_hh_l0 torch.Size([800])\n",
      "model.module.rnn.weight_ih_l1 torch.Size([800, 200])\n",
      "model.module.rnn.weight_hh_l1 torch.Size([800, 200])\n",
      "model.module.rnn.bias_ih_l1 torch.Size([800])\n",
      "model.module.rnn.bias_hh_l1 torch.Size([800])\n",
      "model.module.decoder.weight torch.Size([602755, 200])\n",
      "model.module.decoder.bias torch.Size([602755])\n",
      "\n",
      "Total Parameters: 121,796,955\n"
     ]
    }
   ],
   "source": [
    "setup_torch()\n",
    "# torch.cuda.set_device(1)\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "corpus = get_corpus()\n",
    "\n",
    "# TODO remove these two lines\n",
    "assert len(corpus.dictionary) == 602755\n",
    "assert corpus.valid.size()[0] == 11606861\n",
    "assert corpus.train.max() < len(corpus.dictionary)\n",
    "assert corpus.valid.max() < len(corpus.dictionary)\n",
    "assert corpus.test.max() < len(corpus.dictionary)\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(MODEL_TYPE, ntokens, EMBEDDINGS_SIZE, HIDDEN_UNIT_COUNT, LAYER_COUNT, DROPOUT_PROB,\n",
    "                 TIED).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if use_data_paralellization or USE_DATA_PARALLELIZATION:\n",
    "    model = CustomDataParallel(model)\n",
    "    criterion = DataParallelCriterion(criterion)\n",
    "# else:\n",
    "#     model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "summary(model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Batch Size 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2019-05-30 12:39:27,906: | epoch   1 |   200/11879 batches | lr 20.00 | ms/batch 591.00 | loss  9.79 | ppl 17801.87\n",
      "INFO 2019-05-30 12:41:23,640: | epoch   1 |   400/11879 batches | lr 20.00 | ms/batch 578.66 | loss  8.82 | ppl  6793.58\n",
      "INFO 2019-05-30 12:43:19,524: | epoch   1 |   600/11879 batches | lr 20.00 | ms/batch 579.42 | loss  8.54 | ppl  5130.15\n",
      "INFO 2019-05-30 12:45:15,463: | epoch   1 |   800/11879 batches | lr 20.00 | ms/batch 579.70 | loss  8.41 | ppl  4501.38\n",
      "INFO 2019-05-30 12:47:11,344: | epoch   1 |  1000/11879 batches | lr 20.00 | ms/batch 579.40 | loss  8.25 | ppl  3827.97\n",
      "INFO 2019-05-30 12:49:07,187: | epoch   1 |  1200/11879 batches | lr 20.00 | ms/batch 579.21 | loss  8.19 | ppl  3609.10\n",
      "INFO 2019-05-30 12:51:03,087: | epoch   1 |  1400/11879 batches | lr 20.00 | ms/batch 579.50 | loss  8.06 | ppl  3150.66\n",
      "INFO 2019-05-30 12:52:58,985: | epoch   1 |  1600/11879 batches | lr 20.00 | ms/batch 579.49 | loss  8.05 | ppl  3137.54\n",
      "INFO 2019-05-30 12:54:54,901: | epoch   1 |  1800/11879 batches | lr 20.00 | ms/batch 579.58 | loss  8.02 | ppl  3056.26\n",
      "INFO 2019-05-30 12:56:50,762: | epoch   1 |  2000/11879 batches | lr 20.00 | ms/batch 579.30 | loss  7.89 | ppl  2670.15\n",
      "INFO 2019-05-30 12:58:46,613: | epoch   1 |  2200/11879 batches | lr 20.00 | ms/batch 579.25 | loss  7.92 | ppl  2758.30\n",
      "INFO 2019-05-30 13:00:42,461: | epoch   1 |  2400/11879 batches | lr 20.00 | ms/batch 579.24 | loss  7.89 | ppl  2666.10\n",
      "INFO 2019-05-30 13:02:38,280: | epoch   1 |  2600/11879 batches | lr 20.00 | ms/batch 579.09 | loss  7.90 | ppl  2696.87\n",
      "INFO 2019-05-30 13:04:34,081: | epoch   1 |  2800/11879 batches | lr 20.00 | ms/batch 579.00 | loss  7.88 | ppl  2650.45\n",
      "INFO 2019-05-30 13:06:29,852: | epoch   1 |  3000/11879 batches | lr 20.00 | ms/batch 578.85 | loss  7.84 | ppl  2537.76\n",
      "INFO 2019-05-30 13:08:25,648: | epoch   1 |  3200/11879 batches | lr 20.00 | ms/batch 578.98 | loss  7.81 | ppl  2458.63\n",
      "INFO 2019-05-30 13:10:21,467: | epoch   1 |  3400/11879 batches | lr 20.00 | ms/batch 579.09 | loss  7.79 | ppl  2422.43\n",
      "INFO 2019-05-30 13:12:17,280: | epoch   1 |  3600/11879 batches | lr 20.00 | ms/batch 579.06 | loss  7.76 | ppl  2355.33\n",
      "INFO 2019-05-30 13:14:12,986: | epoch   1 |  3800/11879 batches | lr 20.00 | ms/batch 578.53 | loss  7.77 | ppl  2378.37\n",
      "INFO 2019-05-30 13:16:08,788: | epoch   1 |  4000/11879 batches | lr 20.00 | ms/batch 579.01 | loss  7.77 | ppl  2375.35\n",
      "INFO 2019-05-30 13:18:04,607: | epoch   1 |  4200/11879 batches | lr 20.00 | ms/batch 579.09 | loss  7.72 | ppl  2263.49\n",
      "INFO 2019-05-30 13:20:00,451: | epoch   1 |  4400/11879 batches | lr 20.00 | ms/batch 579.22 | loss  7.75 | ppl  2322.91\n",
      "INFO 2019-05-30 13:21:56,255: | epoch   1 |  4600/11879 batches | lr 20.00 | ms/batch 579.02 | loss  7.72 | ppl  2260.43\n",
      "INFO 2019-05-30 13:23:52,196: | epoch   1 |  4800/11879 batches | lr 20.00 | ms/batch 579.70 | loss  7.73 | ppl  2285.65\n",
      "INFO 2019-05-30 13:25:47,959: | epoch   1 |  5000/11879 batches | lr 20.00 | ms/batch 578.81 | loss  7.73 | ppl  2265.89\n",
      "INFO 2019-05-30 13:27:43,788: | epoch   1 |  5200/11879 batches | lr 20.00 | ms/batch 579.14 | loss  7.70 | ppl  2198.20\n",
      "INFO 2019-05-30 13:29:39,606: | epoch   1 |  5400/11879 batches | lr 20.00 | ms/batch 579.09 | loss  7.67 | ppl  2138.78\n",
      "INFO 2019-05-30 13:31:35,399: | epoch   1 |  5600/11879 batches | lr 20.00 | ms/batch 578.96 | loss  7.65 | ppl  2090.59\n",
      "INFO 2019-05-30 13:33:31,229: | epoch   1 |  5800/11879 batches | lr 20.00 | ms/batch 579.15 | loss  7.63 | ppl  2051.41\n",
      "INFO 2019-05-30 13:35:27,016: | epoch   1 |  6000/11879 batches | lr 20.00 | ms/batch 578.93 | loss  7.64 | ppl  2080.46\n",
      "INFO 2019-05-30 13:37:22,787: | epoch   1 |  6200/11879 batches | lr 20.00 | ms/batch 578.85 | loss  7.65 | ppl  2095.43\n",
      "INFO 2019-05-30 13:37:52,897: -----------------------------------------------------------------------------------------\n",
      "INFO 2019-05-30 13:37:52,897: Exiting from training early\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/trained_models/model-2019-05-30 12:37:29.706331.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2cca7ed578c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/gabrielamelo/Novo volume/Projects/portuguese_wsc/src/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, corpus, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exiting from training early'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/gabrielamelo/Novo volume/Projects/portuguese_wsc/src/training.py\u001b[0m in \u001b[0;36mget_training_results\u001b[0;34m(model, corpus, criterion, device, timestamp)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_training_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# Load the best saved model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_FILE_NAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/trained_models/model-2019-05-30 12:37:29.706331.pt'"
     ]
    }
   ],
   "source": [
    "train(model, corpus, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Batch Size 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabrielamelo/anaconda3/envs/wsc_port/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "INFO 2019-05-29 19:20:33,895: | epoch   1 |   200/11879 batches | lr 20.00 | ms/batch 608.19 | loss  8.52 | ppl  5006.08\n",
      "INFO 2019-05-29 19:22:32,776: | epoch   1 |   400/11879 batches | lr 20.00 | ms/batch 594.40 | loss  7.77 | ppl  2356.94\n",
      "INFO 2019-05-29 19:24:31,470: | epoch   1 |   600/11879 batches | lr 20.00 | ms/batch 593.47 | loss  7.64 | ppl  2072.55\n",
      "INFO 2019-05-29 19:26:30,188: | epoch   1 |   800/11879 batches | lr 20.00 | ms/batch 593.59 | loss  7.66 | ppl  2131.26\n",
      "INFO 2019-05-29 19:28:28,873: | epoch   1 |  1000/11879 batches | lr 20.00 | ms/batch 593.43 | loss  7.64 | ppl  2078.91\n",
      "INFO 2019-05-29 19:30:27,597: | epoch   1 |  1200/11879 batches | lr 20.00 | ms/batch 593.62 | loss  7.62 | ppl  2040.10\n",
      "INFO 2019-05-29 19:32:26,271: | epoch   1 |  1400/11879 batches | lr 20.00 | ms/batch 593.37 | loss  7.54 | ppl  1875.25\n",
      "INFO 2019-05-29 19:34:24,978: | epoch   1 |  1600/11879 batches | lr 20.00 | ms/batch 593.53 | loss  7.57 | ppl  1944.25\n",
      "INFO 2019-05-29 19:36:23,696: | epoch   1 |  1800/11879 batches | lr 20.00 | ms/batch 593.58 | loss  7.56 | ppl  1925.22\n",
      "INFO 2019-05-29 19:38:22,366: | epoch   1 |  2000/11879 batches | lr 20.00 | ms/batch 593.35 | loss  7.44 | ppl  1705.36\n",
      "INFO 2019-05-29 19:40:21,121: | epoch   1 |  2200/11879 batches | lr 20.00 | ms/batch 593.77 | loss  7.51 | ppl  1831.16\n",
      "INFO 2019-05-29 19:42:19,831: | epoch   1 |  2400/11879 batches | lr 20.00 | ms/batch 593.55 | loss  7.51 | ppl  1822.04\n",
      "INFO 2019-05-29 19:44:18,594: | epoch   1 |  2600/11879 batches | lr 20.00 | ms/batch 593.81 | loss  7.57 | ppl  1940.62\n",
      "INFO 2019-05-29 19:45:23,276: -----------------------------------------------------------------------------------------\n",
      "INFO 2019-05-29 19:45:23,276: Exiting from training early\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/trained_models/model-2019-05-29 19:18:32.258388.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2cca7ed578c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/gabrielamelo/Novo volume/Projects/portuguese_wsc/src/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, corpus, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exiting from training early'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/gabrielamelo/Novo volume/Projects/portuguese_wsc/src/training.py\u001b[0m in \u001b[0;36mget_training_results\u001b[0;34m(model, corpus, criterion, device, timestamp)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_training_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;31m# Load the best saved model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_FILE_NAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/trained_models/model-2019-05-29 19:18:32.258388.pt'"
     ]
    }
   ],
   "source": [
    "# with optimizer:\n",
    "train(model, corpus, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp = datetime.datetime.now()\n",
    "# with open(MODEL_FILE_NAME.format(timestamp), 'wb') as f:\n",
    "#     torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(MODEL_FILE_NAME.format(timestamp), 'rb') as f:\n",
    "with open('models/trained_models/model-2019-05-24 17:19:46.971655.pt', 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    model.rnn.flatten_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2019-05-27 10:57:05,109: -----------------------------------------------------------------------------------------\n",
      "INFO 2019-05-27 10:57:05,110: Running eval\n",
      "INFO 2019-05-27 10:57:05,110: -----------------------------------------------------------------------------------------\n",
      "INFO 2019-05-27 10:57:31,470: |  1000/42211 batches | loss 175.47\n",
      "INFO 2019-05-27 10:57:57,898: |  2000/42211 batches | loss 175.93\n",
      "INFO 2019-05-27 10:58:24,412: |  3000/42211 batches | loss 176.24\n",
      "INFO 2019-05-27 10:58:50,994: |  4000/42211 batches | loss 176.12\n",
      "INFO 2019-05-27 10:59:17,711: |  5000/42211 batches | loss 175.86\n",
      "INFO 2019-05-27 10:59:44,515: |  6000/42211 batches | loss 175.90\n",
      "INFO 2019-05-27 11:00:11,298: |  7000/42211 batches | loss 175.92\n",
      "INFO 2019-05-27 11:00:38,091: |  8000/42211 batches | loss 176.16\n",
      "INFO 2019-05-27 11:01:04,898: |  9000/42211 batches | loss 176.03\n",
      "INFO 2019-05-27 11:01:31,771: | 10000/42211 batches | loss 176.04\n",
      "INFO 2019-05-27 11:01:58,692: | 11000/42211 batches | loss 175.98\n",
      "INFO 2019-05-27 11:02:25,636: | 12000/42211 batches | loss 175.79\n",
      "INFO 2019-05-27 11:02:52,621: | 13000/42211 batches | loss 176.28\n",
      "INFO 2019-05-27 11:03:19,625: | 14000/42211 batches | loss 176.09\n",
      "INFO 2019-05-27 11:03:46,666: | 15000/42211 batches | loss 175.91\n",
      "INFO 2019-05-27 11:06:28,953: | 21000/42211 batches | loss 175.49\n",
      "INFO 2019-05-27 11:06:56,057: | 22000/42211 batches | loss 175.48\n",
      "INFO 2019-05-27 11:07:23,091: | 23000/42211 batches | loss 175.46\n",
      "INFO 2019-05-27 11:07:50,256: | 24000/42211 batches | loss 175.49\n",
      "INFO 2019-05-27 11:08:17,323: | 25000/42211 batches | loss 175.52\n",
      "INFO 2019-05-27 11:08:44,392: | 26000/42211 batches | loss 175.58\n",
      "INFO 2019-05-27 11:09:11,445: | 27000/42211 batches | loss 175.67\n",
      "INFO 2019-05-27 11:09:38,418: | 28000/42211 batches | loss 175.73\n",
      "INFO 2019-05-27 11:10:05,249: | 29000/42211 batches | loss 175.78\n",
      "INFO 2019-05-27 11:10:32,054: | 30000/42211 batches | loss 175.86\n",
      "INFO 2019-05-27 11:10:58,873: | 31000/42211 batches | loss 175.93\n",
      "INFO 2019-05-27 11:11:25,678: | 32000/42211 batches | loss 175.88\n",
      "INFO 2019-05-27 11:11:52,473: | 33000/42211 batches | loss 175.90\n",
      "INFO 2019-05-27 11:12:19,316: | 34000/42211 batches | loss 175.94\n",
      "INFO 2019-05-27 11:12:46,221: | 35000/42211 batches | loss 176.05\n",
      "INFO 2019-05-27 11:13:13,208: | 36000/42211 batches | loss 176.09\n",
      "INFO 2019-05-27 11:13:40,229: | 37000/42211 batches | loss 176.02\n",
      "INFO 2019-05-27 11:14:07,277: | 38000/42211 batches | loss 176.02\n",
      "INFO 2019-05-27 11:14:34,259: | 39000/42211 batches | loss 176.14\n",
      "INFO 2019-05-27 11:15:01,315: | 40000/42211 batches | loss 176.25\n",
      "INFO 2019-05-27 11:15:28,366: | 41000/42211 batches | loss 176.22\n",
      "INFO 2019-05-27 11:15:55,734: | 42000/42211 batches | loss 176.29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.038112595037704"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, corpus, criterion, device, use_test_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsc_port",
   "language": "python",
   "name": "wsc_port"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
